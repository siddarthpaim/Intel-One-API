# Intel-One-API
---

Exploring Intel's One API Architecture.
Intel announced OneAPI at the end of 2018 as an open-source, single unified programming model, which is standard-based and delivers a simpler development process across different processor silicon architectures. This gives the leverage to the developers to build high performance cross-platform industry-leading accelerated computing code making use of parallelism across CPUs, GPUs, Neural Engines and FPGAs. This product has been a path breaker to the intel in its short- and long-term strategy. This will also enable a broad ecosystem and partners to build evolutionary products in computing. It has an inherited edge over other proprietary library as it is vendor neutral.
Today, as some of the hardware requires the developers to write a proprietary code using different programming languages, frameworks, and libraries such a need to develop and maintain separate code blocks for each of the hardware reduces the efficiency and pace of innovation of the companies. This also reduces dependence on any of the individual vendors making use of cross-platform parallel integration of the system possible. It has also been one of the world's largest open-source software.
With the advent of growth of Computing needs for AI, machine learning, data centre and edge computing, the industry is needing such a unified standard which optimises developer code across platforms. We have diversity of computing workloads in the industry that are broadly into categories such as scalar, vector, matrix and spatial. We shall discuss each of them one by one. General purpose Central Processing Units such as CPU are scalar which for every cycle take 1 instruction on 1 word of data. Graphic processing units such as GPU are specialised Integrated circuits able to take complex workload for processing Images and videos, they are designed to provide high throughput as they take for every cycle many words of data. GPUs leverage the data level parallelism because of their executing concurrent processes at the same time. Matrix or Tensor processing Units or popularly known as TPUs are those hardware accelerators that for every cycle take 1 instruction and multiplies matrices such as 2*4, 4*2 to achieve output matrix of size 2*2. They are useful for deep learning tasks. As neural networks require complex matrix multiplication between various data and parameters TPUs help in achieving them with very less power and time. Another type of computer architecture is FPGA (Field Programmable Gate Array) or Spatial Computing they are a specialised semiconductor device that are built as matrix configurable blocks connected via interconnects. They are having lot of applications in Augmented reality, virtual reality, self-driving cars and many more. In Contrast to traditional CPUs and GPUs where the instruction flows through data pipelines here the data flows through grid. They offer superior performance in Deep Neural networks (DNNs) with great energy efficiency. They also offer better flexibility and rapid prototyping.
As these different computer architectures define the foundation of computing then, the system programming is a layer that comes above it these are the software that are part of operating systems, they are like peer-to-peer messaging platforms, task scheduler, sync primitives, Device Profiler, Device and Memory Management, Tracing and debugging tools etc. Their functional improvements can greatly enhance the performance of subsequent layer of workloads they play a significant role hence especially when the workload is of the size is large like that of data centre or telecom tower or other HPC workload.

---

Delivering High-performance computing in today's age requires resilient computer architecture that is highly efficient, optimized, and scalable. Due to the advent of machine learning and artificial intelligence, the demand for compute-intensive tasks has been growing at an enormous pace. As today's diverse set of compute architecture as explained above require companies to make significant investments into hardware, libraries, frameworks, and tools there is an increasing need for reusability across the different architecture. In my undergrad, I made one presentation on the need for ubiquitous computing in the modern era of computing. Intel's One API helps simplify the programming for CPUs and hardware and neural accelerators across architectures to seamlessly exploit parallelism called Data-Parallel C++. The DPC++ enables code reuse across CPUs and GPUs with a single source code. DPC++ also has a mapping within to make use of hardware accelerations across the devices as and when needed.
Data-Parallel C++ also known as the DPC++ is a high-level language specially designed for highly intensive data-parallel programming and productivity. It reuses C++ and enhances it with its cross-platform compatibility.

Device Selection and Offloading
The user can select a device and offload the task to hardware such as CPU, GPU, TPU etc can be done by mentioning it in the code. Currently, it is available for both DCP++ and Open MP applications.
The device can be selected in the host code by explicitly mentioning the device type. We can initialize the device with the following
1) default_selector: When this is used, the kernel makes use of and runs on all available computing devices.
2) cpu_selector: When this is used, the kernel uses only the CPU available in the system
3) gpu_selector: When this is used, the kernel uses only the GPU available in the system
4) accelerator_selector: When this is used the kernel uses an accelerator that is available in the system.
The device throws AOT or ahead of time compiled binary error when it is run, and the platform does not contain a given type of device.
After the Code is written the User wants to compile and Run the Program. I would like to explain how it is done in Open API. We know that One API uses Single Source Compilation, which is a programming technique used in C and C++ languages this reduces compilation time spanning multiple files (generally host and device compilation). Also, Open API gives the user flexibility to choose a different Host and device compilation as some developers prefer it that way. Some of the advantages of single-source compilation are 
1) Usability: Developers can create fewer files and define code with flexibility
2) Extra Safety: Compiler can differentiate between host and device code the code generated by the host compiler will match that generated by the kernel.
3) Optimisation: The device compiler can perform custom optimization by knowing the context of invocation. 
Invoking the Compiler: The compiler is invoked in the DPC++ using the command dpcpp 
The compiler has different compatibility with different operating systems. It recognises GCC-style command-line options as well as windows type command-line options.
Thus, we have seen a detailed overview of Intel's Open API architecture and its application, which uses over traditional compilers. It can also be accessed without any installation through Intel Devcloud.
